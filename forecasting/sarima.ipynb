{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666519ad-6b16-4a72-a204-32e2044e43d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install matplotlib numpy pmdarima sklearn statsmodels pandas plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5306382b-8d3c-45a9-ac9d-b3d27408b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from plotly import graph_objs as go\n",
    "from pmdarima.arima.utils import ndiffs\n",
    "import logging\n",
    "from pmdarima.arima import StepwiseContext\n",
    "from data_util_common import train_val_test_split, show_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540228a8-a632-4e6b-935c-b5908c011fde",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def check_stationary(data, lags):\n",
    "    rolling_mean = data.rolling(window=lags).mean().dropna()\n",
    "    rolling_std = data.rolling(window=lags).std()\n",
    "    plt.plot(data, color='blue', label='Original')\n",
    "    plt.plot(rolling_mean, color='red', label='Rolling Mean')\n",
    "    plt.plot(rolling_std, color='black', label='Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    dtFmt = mdates.DateFormatter('%Y-%b')  # define the formatting\n",
    "    plt.gca().xaxis.set_major_formatter(dtFmt)\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "    plt.xticks(rotation=0, fontweight='light', fontsize='x-small')\n",
    "    plt.title('Rolling Mean & Rolling Standard Deviation')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def kpss_test(series, **kw):\n",
    "    statistic, p_value, n_lags, critical_values = kpss(series, **kw)\n",
    "    print(\n",
    "        'KPSS checks for stochastic trend (e.g., which does not have a constant variance throughout time) stationary.')\n",
    "    print(f'KPSS Statistic: {statistic}')\n",
    "    print(f'p-value: {p_value}')\n",
    "    print(f'num lags: {n_lags}')\n",
    "    print('Critical Values:')\n",
    "    for key, value in critical_values.items():\n",
    "        print(f'   {key} : {value}')\n",
    "    print(f'Result: The series is {\"not \" if p_value < 0.05 else \"\"}stationary')\n",
    "    if p_value < 0.05:\n",
    "        n_kpss = ndiffs(series, test='kpss')\n",
    "        print(f'N steps required for differencing: {n_kpss}')\n",
    "    print()\n",
    "\n",
    "\n",
    "def adf_test(series, **kw):\n",
    "    result = adfuller(series, **kw)\n",
    "    print('ADF checks for unit root presence.')\n",
    "    print('ADF Statistic: {}'.format(result[0]))\n",
    "    print('p-value: {}'.format(result[1]))\n",
    "    print(f'num lags: {result[2]}')\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t{}: {}'.format(key, value))\n",
    "    print(f'Result: The series is {\"not \" if result[1] > 0.05 else \"\"} stationary')\n",
    "    if result[1] > 0.05:\n",
    "        n_adf = ndiffs(series, test='adf')\n",
    "        print(f'N steps required for differencing: {n_adf}')\n",
    "    print()\n",
    "\n",
    "\n",
    "def differencing(data, periods):\n",
    "    data_diff = data.diff(periods)\n",
    "    data_diff.dropna(inplace=True)\n",
    "    return data_diff\n",
    "\n",
    "\n",
    "def remove_seasonality(data, lag):\n",
    "    seasonal_diff = data - data.shift(lag)\n",
    "    seasonal_diff = seasonal_diff.dropna(inplace=False)\n",
    "    return seasonal_diff\n",
    "\n",
    "\n",
    "def show_result(df, title, x_title, y_title):\n",
    "    fig = go.Figure()\n",
    "    for col in df.columns:\n",
    "        fig.add_trace(go.Bar(x=df.index, y=df[col], name=col, opacity=0.7))\n",
    "    fig.update_layout(barmode='overlay')\n",
    "    fig.update_layout(title=title)\n",
    "    fig.update_xaxes(title=x_title)\n",
    "    fig.update_yaxes(title=y_title)\n",
    "    fig.write_html('sarima_result.html')\n",
    "    fig.show()\n",
    "    \n",
    "def canopy_dataset():\n",
    "    filename = 'data/NIST_Canopy_2015_2018_interpolated.csv'\n",
    "    label = 'ACP_kW'\n",
    "    data = pd.read_csv(filename, sep=',')\n",
    "    data['TIMESTAMP'] = pd.to_datetime(data['TIMESTAMP'], format='%Y-%m-%d %H:%M:%S')\n",
    "    data.set_index(data['TIMESTAMP'], drop=True, inplace=True)\n",
    "    # data = filter_data(data, \"2015-01-01\", \"2018-01-01\")\n",
    "\n",
    "    data[label] = data[label].astype(float)\n",
    "    date = [x.strftime('%Y-%m-%d') for x in pd.to_datetime(data.index).date]\n",
    "    time = [x.strftime('%H:%M:%S') for x in pd.to_datetime(data.index).time]\n",
    "\n",
    "    print(data.columns)\n",
    "    print(\"Correlation ----------------------------------------------------------------------------------------------\")\n",
    "    corr_data = data.drop(columns=[label, 'season']).corrwith(data[label])\n",
    "    print(corr_data)\n",
    "\n",
    "    pv_per_hour = data[[label]].copy()\n",
    "    pv_per_hour['Time'] = time\n",
    "    pv_per_hour = pv_per_hour.groupby(\"Time\").agg({label: list}).reset_index()\n",
    "    pv_per_hour = pv_per_hour.drop(columns=[\"Time\"])\n",
    "    show_box(pv_per_hour, label, \"Energy distribution per hour\", \"Time [h]\", \"Energy [Wh]\")\n",
    "    del pv_per_hour\n",
    "    return data, date, time, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75348d-4e03-45e1-91ed-b973f775bb4b",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "format = \"%Y-%m-d\"\n",
    "T = 24\n",
    "data, date, time, label = canopy_dataset()\n",
    "features = ['Irradiance_1_Wm2']\n",
    "data = data[[label] + features]\n",
    "\n",
    "date_time = data.index\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# data = differencing(data, 1)\n",
    "# data = remove_seasonality(data, 24)\n",
    "\n",
    "print(\"**AUTOCORRELATION ** -------------------------------------------------------------------------------------\")\n",
    "# px.line(x=data.index, y=data[label], labels={'x': 'Time', 'y': label}).show()\n",
    "# show_result(data, 'PV plant production', 'Time', label)\n",
    "\n",
    "print(\"The autocorrelation fu nction (ACF) assesses the correlation between observations in a time series for a\\n\"\n",
    "      \"set of lags. In an ACF plot, each bar represents the size and direction of the correlation. Bars that\\n\"\n",
    "      \"extend across the light blue area are statistically significant. When seasonal patterns are present,\\n\"\n",
    "      \"the autocorrelations are larger for lags at multiples of the seasonal frequency than for other lags.\")\n",
    "plot_acf(data[label], lags=T)\n",
    "plt.show()\n",
    "\n",
    "print(\"The partial autocorrelation function is similar to the ACF except that it displays only the correlation\\n\"\n",
    "      \"between two observations that the shorter lags between those observations do not explain. The most \\n\"\n",
    "      \"statistically significant lags suggests the order of a autoregressive model\")\n",
    "\n",
    "plot_pacf(data[label], lags=T)\n",
    "plt.show()\n",
    "\n",
    "print(\"** STATIONARITY ** ---------------------------------------------------------------------------------------\")\n",
    "# check_stationary(data, lags)\n",
    "adf_test(data[label], maxlag=T, autolag=None, regression='ct')\n",
    "kpss_test(data[label], nlags=T, regression='ct')\n",
    "\n",
    "print(\"** TRAINING ** -------------------------------------------------------------------------------------------\")\n",
    "# Create the SARIMA model\n",
    "# p: The number of lag observations included in the model, also called the lag order.\n",
    "# d: The number of times that the raw observations are differenced, also called the degree of differencing.\n",
    "# q: The size of the moving average window, also called the order of moving average.\n",
    "\n",
    "# The (P,D,Q,M) Order refers to the seasonal component of the model for the Auto Regressive parameters,\n",
    "# differences, Moving Average parameters, and periodicity: D indicates the integration order of the seasonal\n",
    "# process (the number of transformation needed to make stationary the time series). P indicates the Auto\n",
    "# Regressive order for the seasonal component. Q indicated the Moving Average order for the seasonal component. M\n",
    "# indicates the periodicity, i.e. the number of periods in season, such as 12 for monthly data.\n",
    "\n",
    "\n",
    "split = train_val_test_split(date_time, percentage=[80, 20])\n",
    "train = data[label][:split[0]]\n",
    "test = data[label][split[0]:].values\n",
    "exogenous_train = data[features][:split[0]]\n",
    "exogenous_test = data[features][split[0]:].values\n",
    "test_size = len(test)\n",
    "train_size = len(train)\n",
    "print(f\"TRAIN SIZE {train_size} TEST SIZE {test_size}\")\n",
    "\n",
    "# EXOGENOUS DATA\n",
    "sarima=None\n",
    "param_search=True\n",
    "if param_search:\n",
    "    with StepwiseContext(max_dur=30):\n",
    "        sarima = auto_arima(train.values,\n",
    "                            X=exogenous_train,\n",
    "                            test='adf',\n",
    "                            d=None,\n",
    "                            seasonal=True,\n",
    "                            D=None,\n",
    "                            m=T,\n",
    "                            trace=True,\n",
    "                            error_action='ignore',\n",
    "                            suppress_warnings=True,\n",
    "                            approximation=True,\n",
    "                            stepwise=True,\n",
    "                            maxiter=20,\n",
    "                            method='cg'\n",
    "                            )\n",
    "    \n",
    "    predictions = abs(sarima.predict(n_periods=len(test), X=exogenous_test))\n",
    "else:\n",
    "    # MANUAL SARIMA\n",
    "    order = (2, 0, 2)  # (p, d, q)\n",
    "    seasonal_order = (1, 0, 1, T)  # (P, D, Q, s)\n",
    "    \n",
    "    sarima = sm.tsa.SARIMAX(endog=train,\n",
    "                                exog=exogenous_train,\n",
    "                                order=order,\n",
    "                              seasonal_order=seasonal_order\n",
    "                              ).fit(low_memory=True)\n",
    "    \n",
    "    predictions = sarima.predict(\n",
    "            exog=exogenous_test,\n",
    "            start=train_size, end=train_size+test_size-1)\n",
    "\n",
    "print(\"** TESTING ** --------------------------------------------------------------------------------------------\")\n",
    "print(sarima.summary())\n",
    "\n",
    "print(f\"LOOKBACK\\n\"\n",
    "      f\"MAE: {mean_absolute_error(test, data[label].values[train_size - T:-T])} \")\n",
    "\n",
    "print(f\"SARIMA\\n\"\n",
    "      f\"MAE: {mean_absolute_error(test, predictions)} \")\n",
    "\n",
    "results = pd.DataFrame().from_dict({\"Y_True\": test, \"Y_Pred\": predictions})\n",
    "results.index = date_time[train_size:]\n",
    "results.to_csv(\"sarima/results.csv\", sep=\",\")\n",
    "show_result(results, \"SARIMAX forecasting\", \"Time [h]\", \"Energy [Wh]\", 'sarima/sarima_results.html')\n",
    "\n",
    "logging.info(\"SAVING MODEL....\")\n",
    "with open('sarima.pkl', 'wb') as pkl:\n",
    "    pickle.dump(sarima, pkl)\n",
    "logging.info(\"Done.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b60b3-e8c1-40a5-9241-4fd120077fa8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sarimax",
   "language": "python",
   "name": "sarimax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
