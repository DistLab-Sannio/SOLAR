{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UNCOMMENT IF ON REMOTE JUPYTER\n",
    "# !pip uninstall pmdarima statsmodels matplotlib --yes\n",
    "# !pip install protobuf==3.20.0 tensorflow==2.6.2 plotly pandas optuna"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys, os\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras import Input, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from keras.layers import LeakyReLU\n",
    "from data_util_common import canopy_dataset, train_val_test_split\n",
    "from lstm_util import prepare_data, evaluate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "525eebcd5d5749d2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "targets = logging.StreamHandler(sys.stdout), logging.FileHandler('lstm.log')\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO, handlers=targets)\n",
    "format = \"%Y-%m-%d\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60c3b1b39e710f98"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# DEFINE OPTUNA TRIAL\n",
    "def objective(trial):   \n",
    "    models = dict()\n",
    "    models[1] = [168, 48, 24]\n",
    "    models[2] = [168, 72, 48, 24]\n",
    "    models[3] = [24, 24, 24]\n",
    "      \n",
    "    idx = trial.suggest_categorical(\"model\", [x for x in range(1, len(models))])\n",
    "    bidirectional = trial.suggest_categorical(\"bidirectional\", [True, False])\n",
    "    dense_activation = trial.suggest_categorical(\"dense_act\", ['relu', 'leaky relu', 'linear', 'tanh'])\n",
    "\n",
    "    for batch in dataset_train.take(1):\n",
    "        inputs, targets = batch\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input((inputs.shape[1], inputs.shape[2]), None))\n",
    "    for unit in models[idx]:\n",
    "       if bidirectional:\n",
    "            model.add(Bidirectional(LSTM(unit, return_sequences=True)))\n",
    "       else:\n",
    "          model.add(LSTM(unit, return_sequences=True))\n",
    "\n",
    "    if dense_activation == 'leaky relu':\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.add(LeakyReLU())\n",
    "    else:\n",
    "       model.add(TimeDistributed(Dense(1, activation=dense_activation)))\n",
    "    \n",
    "    model.compile(optimizer=Nadam(learning_rate=0.0001), loss=\"mae\")\n",
    "\n",
    "    checkpoint_filepath = f'optimization/checkpoint_{trial.number}.h5'\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, \n",
    "                        monitor=\"val_loss\",\n",
    "                        save_best_only=True, verbose=0)\n",
    "    \n",
    "    history = model.fit(\n",
    "        dataset_train,\n",
    "        epochs=1000,\n",
    "        validation_data=dataset_val,\n",
    "        shuffle=False,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", patience=30, min_delta=0.01), checkpoint],\n",
    "        verbose=0\n",
    "    )\n",
    "     \n",
    "    model.load_weights(checkpoint_filepath)\n",
    "   \n",
    "    days = int((test_size-past)/T)\n",
    "    results = evaluate(dataset_test, model, days, pipeline.named_transformers_['label'])        \n",
    "    mae = mean_absolute_error(results['y_true'], results['y_pred'])\n",
    "    return mae\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1407ac1ec54f673e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data, label = canopy_dataset()\n",
    "data = data[[label, \n",
    "            'Irradiance_1_Wm2'\n",
    "            ]]\n",
    "\n",
    "print(data.columns)\n",
    "label_idx = list(data.columns).index(label)\n",
    "print(label_idx)\n",
    "\n",
    "T = 24\n",
    "past = T * 7 \n",
    "future = T\n",
    "step = 1\n",
    "batch_size = 512\n",
    "sequence_length = int(past / step)\n",
    "\n",
    "print(f\"RESOLUTION: {T} - LOOKBACK WINDOW SIZE: {past} - PREDICTION WINDOW SIZE: {future}\")\n",
    "\n",
    "# SPLIT DATA IN TRAIN AND TEST\n",
    "split_idx = train_val_test_split(pd.to_datetime(data.index), [80, 20])\n",
    "train_val_size = split_idx[0]\n",
    "train_val_split = train_val_test_split(data.index[:train_val_size], [70, 30])\n",
    "train_size = train_val_split[0]\n",
    "val_size = train_val_size - train_size\n",
    "\n",
    "train_val_size = split_idx[0]\n",
    "train = data.iloc[:train_val_size, :]\n",
    "\n",
    "# scale data\n",
    "numeric_features=['Irradiance_1_Wm2']\n",
    "\n",
    "features_scaler = MinMaxScaler()\n",
    "label_scaler = MinMaxScaler()\n",
    "\n",
    "pipeline=ColumnTransformer([\n",
    "    ('label', MinMaxScaler(), [label]),\n",
    "    ('num', MinMaxScaler(), numeric_features),\n",
    "], remainder='passthrough')\n",
    "\n",
    "# SPLIT DATA IN TRAIN AND TEST\n",
    "split_idx = train_val_test_split(pd.to_datetime(data.index), [80, 20])\n",
    "train_val_size = split_idx[0]\n",
    "train = data.iloc[:train_val_size, :]\n",
    "\n",
    "train=pipeline.fit_transform(train)\n",
    "\n",
    "# SPLIT TRAIN IN SUB-TRAIN AND VALIDATION\n",
    "train_val_split = train_val_test_split(data.index[:train_val_size], [70, 30])\n",
    "train_size = train_val_split[0]\n",
    "val_size = train_val_size - train_size\n",
    "\n",
    "val = train[train_size:, :]\n",
    "train = train[:train_size, :]\n",
    "\n",
    "# GET TEST VALUES\n",
    "test = data.iloc[train_val_size:, :]\n",
    "test = pipeline.transform(test)\n",
    "test_size = len(test)\n",
    "test_timestamp = data.index[train_val_size:]\n",
    "print(f\"TRAIN {train_size} VAL {val_size} TEST {test_size} - TOTAL {len(data)}\")\n",
    "\n",
    "#####################################################################################################################\n",
    "# PREPARE DATA\n",
    "# TRAIN\n",
    "dataset_train = prepare_data(train, 0, train_size-past, past, train_size, sequence_length, 1, batch_size, label_idx)\n",
    "\n",
    "# VAL\n",
    "dataset_val = prepare_data(val, 0, val_size-past, past, val_size, sequence_length, 1, batch_size, label_idx)\n",
    "\n",
    "# TEST\n",
    "dataset_test = prepare_data(test, 0, test_size, past, test_size, sequence_length, 1, batch_size, label_idx)\n",
    "\n",
    "print(\"*** TRAINING *** --------------------------------------------------------------------------------------\")\n",
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "\n",
    "print(f\"Input shape:  {inputs.numpy().shape}\")\n",
    "print(f\"Target shape: {targets.numpy().shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44e7f6dd16a41920"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#OPTUNA TRIALS\n",
    "with tf.device('/gpu:0'):\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_jobs=2, show_progress_bar=True, n_trials=50)\n",
    "\n",
    "with open(\"optimization_Res.txt\", \"w\") as f:\n",
    "    f.write(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "    f.write(\"Best trial:\")\n",
    "    \n",
    "    trial = study.best_trial\n",
    "    f.write(\"  Value: {}\".format(trial.value))\n",
    "    \n",
    "    f.write(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(\"    {}: {}\".format(key, value))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d29b0ee521d479ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
